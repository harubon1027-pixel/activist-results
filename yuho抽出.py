import os
import pandas as pd
import requests, zipfile, io
from datetime import datetime, timedelta

# ===============================
# è¨­å®š
# ===============================
API_KEY = "b3179bcab14b4053a5928dce030612f3"  # ğŸ‘ˆ ãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆã¨åŒã˜
BASE_URL = "https://api.edinet-fsa.go.jp/api/v2"
OUTPUT_DIR = "yuho"

os.makedirs(OUTPUT_DIR, exist_ok=True)

# siryou ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
CSV_DIR = "siryou"
os.makedirs(CSV_DIR, exist_ok=True)
CSV_PATH = os.path.join(CSV_DIR, "EdinetcodeDlInfo.csv")

# ===== å¯¾è±¡æ§˜å¼ã‚³ãƒ¼ãƒ‰ï¼ˆæœ‰å ±/åŠæœŸ/å››åŠæœŸï¼‰ =====
YUHO_ORD_FORM = {
    ("010", "030000"), # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
    ("010", "100000"), # åŠæœŸå ±å‘Šæ›¸
}

def is_target_report(doc: dict, edinet_codes: set) -> bool:
    """EDINETã‚³ãƒ¼ãƒ‰ï¼‹æ§˜å¼ã‚³ãƒ¼ãƒ‰ã§å¯¾è±¡æ›¸é¡ã‹åˆ¤å®š"""
    ord_code = str(doc.get("ordinanceCode") or "").zfill(3)
    form_code = str(doc.get("formCode") or "").zfill(6)
    edinet_code = doc.get("edinetCode")
    return edinet_code in edinet_codes and (ord_code, form_code) in YUHO_ORD_FORM

def get_documents_list(date_str):
    """æ—¥ä»˜ã‚’æŒ‡å®šã—ã¦æå‡ºæ›¸é¡ä¸€è¦§ã‚’å–å¾—"""
    url = f"{BASE_URL}/documents.json"
    params = {"date": date_str, "type": 2, "Subscription-Key": API_KEY}
    res = requests.get(url, params=params, timeout=30)
    if res.status_code != 200:
        print(f"âŒ æ›¸é¡ä¸€è¦§å–å¾—å¤±æ•—: {date_str}, HTTP {res.status_code}")
        return []
    return res.json().get("results", [])

def download_xbrl(doc_id):
    """docID ã‚’æŒ‡å®šã—ã¦ZIPã‚’ä¿å­˜ï¼†è§£å‡"""
    try:
        url = f"{BASE_URL}/documents/{doc_id}"
        params = {"type": 1, "Subscription-Key": API_KEY}
        res = requests.get(url, params=params, timeout=60)
        res.raise_for_status()

        save_path = os.path.join(OUTPUT_DIR, f"{doc_id}.zip")
        with open(save_path, "wb") as f:
            f.write(res.content)

        with zipfile.ZipFile(io.BytesIO(res.content)) as z:
            z.extractall(os.path.join(OUTPUT_DIR, doc_id))

        print(f"âœ… {doc_id}.zip ä¿å­˜ï¼†è§£å‡å®Œäº†")
    except Exception as e:
        print(f"âŒ {doc_id} ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

def prepare_edinet_code_csv():
    """EdinetcodeDlInfo.csv ãŒç„¡ã‘ã‚Œã°è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    if not os.path.exists(CSV_PATH):
        url = "https://disclosure2dl.edinet-fsa.go.jp/EdinetcodeDlInfo.csv"
        res = requests.get(url, timeout=30)
        if res.status_code == 200:
            with open(CSV_PATH, "wb") as f:
                f.write(res.content)
            print(f"âœ… EdinetcodeDlInfo.csv ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
        else:
            raise RuntimeError(f"âŒ EdinetcodeDlInfo.csv ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {res.status_code}")

def main():
    # 1. ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆéŠ˜æŸ„ä¸€è¦§ï¼ˆè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã«æœ«å°¾0ã‚’è¿½åŠ ã—ã¦5æ¡åŒ–ï¼‰
    activist_path = os.path.join("docs", "ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆéŠ˜æŸ„ä¸€è¦§.xlsx")
    activist_df = pd.read_excel(activist_path)
    target_sec_codes = set(str(c).zfill(4) + "0" for c in activist_df["è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰"].dropna().unique())

    # 2. EdinetcodeDlInfo.csv ã‚’ç”¨æ„ã—ã¦èª­ã¿è¾¼ã¿
    prepare_edinet_code_csv()
    code_df = pd.read_csv(CSV_PATH, encoding="cp932", skiprows=1)

    # 3. åˆ—åã§æŒ‡å®šï¼ˆCSVå´ã¯ã™ã§ã«5æ¡ï¼‰
    edinet_col = code_df["ï¼¥ï¼¤ï¼©ï¼®ï¼¥ï¼´ã‚³ãƒ¼ãƒ‰"]
    sec_col = code_df["è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰"].astype(str).str.strip()

    # 4. ãƒãƒƒãƒãƒ³ã‚°
    edinet_codes = set(edinet_col[sec_col.isin(target_sec_codes)].tolist())
    print(f"ğŸ¯ å¯¾è±¡EDINETã‚³ãƒ¼ãƒ‰: {len(edinet_codes)}ä»¶")

    # 5. ç›´è¿‘1å¹´é–“ã®æå‡ºæ›¸é¡ã‚’æ¢ç´¢
    today = datetime.now().date()
    days = 365
    for i in range(days):
        date_str = (today - timedelta(days=i)).strftime("%Y-%m-%d")
        docs = get_documents_list(date_str)
        for doc in docs:
            if is_target_report(doc, edinet_codes):
                doc_id = doc["docID"]
                desc = doc.get("docDescription") or ""
                print(f"ğŸ“Œ Hit: {doc_id}, {desc}")
                download_xbrl(doc_id)

if __name__ == "__main__":
    main()
